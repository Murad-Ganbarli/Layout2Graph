name: base
model:
  model_path: /models
  type: resnet18
  mixed_precision: "no" # "["no", "fp16", "bf16"]
  # Whether to use mixed precision from accelerator. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >="
  #  " 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the"
  # " flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config."
predictor:
  img_paths:
    -
  img_dirs:
    -
  save_dir: /data/data/cache
  do_visualize: true
evaluator:
  save_dir: /data/data/cache
trainer:
  resume_flag: false
  # gradient configuration
  grad_accumulate: 1 # gradient accumulation
    # this is also an accelerator api. somehow improve the efficiency; conflict with my implementation of gradient accumulation
    # if you want to use accelerator gradient accumulation, remember set trainer.grad_accumulate = 1
    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate which means
  # you cannot use it if you want to train unet and text_encoder simultaneously
  # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.
  random_seed: ~
  grad_clip:
  loss:
    type: CrossEntropyLoss
  optimizer:
    optimizer_type: "sgd"
    lr: 0.01 # lr0 initial learning rate (SGD=1E-2 Adam=1E-3)
    scale_lr: false  # Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size
    weight_decay: 0.0005  # optimizer weight decay 5e-4
    momentum: 0.937  # SGD momentum/Adam beta1
  scheduler:
    scheduler_type: "linear"
    warmup_epochs: 1
  epochs: 100
  # tensorboard configuration
  save_dir: /home/ysocr/data/workspace/base
  tensorboard_dir: /home/ysocr/data/workspace/base/tensorboard
  # display configuration
  save_epoch_freq: 1
  save_step_freq: 0
  print_freq: 50
  eval_print_freq: 20
  display_freq: 200
  eval_display_freq: 20
datasets:
  train:
    dataset:
      type: BaseDataset
      data_root:
        -
    num_workers: 8
    batch_size: 64
    shuffle: true
    collate_fn:
      type:
  eval:
    dataset:
      data_root:
        -
    shuffle: false
